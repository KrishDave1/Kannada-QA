{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a8YiwTdO6xWt",
    "outputId": "6d2b9f71-7500-4b67-d7cd-894b65fcb35d"
   },
   "outputs": [],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uFN8v5QU7JAT",
    "outputId": "1daa271a-442c-4f9b-b5fb-43973a4bb61e"
   },
   "outputs": [],
   "source": [
    "# !pip install openai-whisper\n",
    "!pip install torch torchvision --index-url https://download.pytorch.org/whl/cu117\n",
    "!pip install pymongo[srv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2f7HDd9A7af3",
    "outputId": "90f28079-b2ee-43c0-b691-a32b31533782"
   },
   "outputs": [],
   "source": [
    "# !pip install langchain_community\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l8nDh9MF7_Xp"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FDvcNt-J8CHf",
    "outputId": "a478aba5-27b6-40bd-cd5a-a988e715670e"
   },
   "outputs": [],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "myGvTfMt8GGN",
    "outputId": "b97a8745-7bb7-46b8-fa93-ebb402cc9168"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "# Authenticate using the token from the environment variable\n",
    "login(token=os.getenv(\"HF_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "kqu6r5iJ8MUZ",
    "outputId": "022a2c0e-17e3-44cb-c4ff-80f2d3a58a96"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from pymongo.mongo_client import MongoClient\n",
    "from pymongo.server_api import ServerApi\n",
    "from urllib.parse import quote_plus\n",
    "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import MongoDBAtlasVectorSearch\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from transformers import pipeline\n",
    "import whisper\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "\n",
    "# MongoDB setup\n",
    "username = quote_plus(\"valmik0000000\")\n",
    "password = quote_plus(\"valmik@mongo7\")  # Replace with your actual password\n",
    "\n",
    "uri = f\"mongodb+srv://{username}:{password}@valmikcluster0.hdqee.mongodb.net/?retryWrites=true&w=majority&appName=ValmikCluster0\"\n",
    "client = MongoClient(uri, server_api=ServerApi('1'), tls=True)\n",
    "\n",
    "try:\n",
    "    client.admin.command('ping')\n",
    "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Feedback file\n",
    "FEEDBACK_FILE = \"feedback.json\"\n",
    "\n",
    "# Load or initialize feedback data\n",
    "if os.path.exists(FEEDBACK_FILE):\n",
    "    with open(FEEDBACK_FILE, \"r\") as f:\n",
    "        feedback_data = json.load(f)\n",
    "else:\n",
    "    feedback_data = {}\n",
    "\n",
    "# Save feedback to file\n",
    "def save_feedback_to_file():\n",
    "    with open(FEEDBACK_FILE, \"w\") as f:\n",
    "        json.dump(feedback_data, f, indent=4)\n",
    "\n",
    "# Database and collection setup for QA\n",
    "dbName = \"ML_Fiesta\"\n",
    "collectionName = \"translations\"\n",
    "collection = client[dbName][collectionName]\n",
    "\n",
    "# Define the embedding model\n",
    "embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "\n",
    "# Check if the data already exists in the vector store\n",
    "if collection.estimated_document_count() == 0:\n",
    "    print(\"No data found in the vector store. Loading documents and creating the vector store.\")\n",
    "    loader = DirectoryLoader(\n",
    "        r\"/content/drive/MyDrive/ML_Fiesta_Mongo_DB_Vectorization/translations\",\n",
    "        glob=\"./*.txt\",\n",
    "        show_progress=True\n",
    "    )\n",
    "    data = loader.load()\n",
    "    vectorStore = MongoDBAtlasVectorSearch.from_documents(data, collection=collection, embedding=embeddings)\n",
    "else:\n",
    "    print(\"Data already exists in the vector store. Connecting to existing vector store.\")\n",
    "    vectorStore = MongoDBAtlasVectorSearch(collection=collection, embedding=embeddings)\n",
    "\n",
    "# Define the question-answering pipeline\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"bert-large-uncased-whole-word-masking-finetuned-squad\", tokenizer=\"bert-large-uncased-whole-word-masking-finetuned-squad\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load Whisper model for audio transcription\n",
    "whisper_model = whisper.load_model(\"base\", device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Global variables to store the last query and answer\n",
    "last_query = \"\"\n",
    "last_answer = \"\"\n",
    "\n",
    "def transcribe_audio(audio_file):\n",
    "    \"\"\"\n",
    "    Transcribes the given audio file using Whisper.\n",
    "    \"\"\"\n",
    "    result = whisper_model.transcribe(audio_file)\n",
    "    return result['text']\n",
    "\n",
    "def query_data(query) -> str:\n",
    "    \"\"\"\n",
    "    Querying data loaded in MongoDB and returning an answer.\n",
    "    \"\"\"\n",
    "    # Check for feedback corrections in the feedback file\n",
    "    if query in feedback_data:\n",
    "        print(\"Using corrected answer from feedback file.\")\n",
    "        return feedback_data[query]  # Use the corrected answer\n",
    "\n",
    "    # If no corrections, use the vector store and QA pipeline\n",
    "    retriever = vectorStore.as_retriever(search_kwargs={\"K\": 61})\n",
    "    retrieved_docs = retriever.get_relevant_documents(query)\n",
    "    context = \" \".join([doc.page_content for doc in retrieved_docs])\n",
    "    response = qa_pipeline(question=query, context=context)\n",
    "    return response['answer']\n",
    "\n",
    "def handle_query(audio):\n",
    "    \"\"\"\n",
    "    Handles user queries from audio input and returns results.\n",
    "    \"\"\"\n",
    "    global last_query, last_answer\n",
    "    # Transcribe the audio input\n",
    "    query = transcribe_audio(audio)\n",
    "    last_query = query  # Store the query for later feedback\n",
    "    answer = query_data(query)\n",
    "    last_answer = answer  # Store the answer for later correction\n",
    "\n",
    "    # Return the question and the answer\n",
    "    return f\"Question: {query}\\nAnswer: {answer}\"\n",
    "\n",
    "# def submit_feedback(feedback_text):\n",
    "#     \"\"\"\n",
    "#     Handles feedback submission and updates the answer if the feedback indicates a wrong answer.\n",
    "#     \"\"\"\n",
    "#     global last_query, last_answer\n",
    "#     if last_query == \"\":\n",
    "#         return \"No query has been submitted yet. Please submit a query first.\"\n",
    "\n",
    "#     if feedback_text.lower() in ['wrong', 'incorrect', 'not correct']:\n",
    "#         # Prompt the user for the correct answer\n",
    "#         corrected_answer = input(f\"Please provide the correct answer for the question '{last_query}': \")\n",
    "#         # Store the feedback and corrected answer in the feedback file\n",
    "#         feedback_data[last_query] = corrected_answer\n",
    "#         save_feedback_to_file()\n",
    "#         print(\"Feedback stored successfully! Answer has been corrected.\")\n",
    "#         return f\"The answer has been corrected. The new answer is: {corrected_answer}\"\n",
    "#     else:\n",
    "#         # Store feedback without corrections\n",
    "#         feedback_data[last_query] = last_answer\n",
    "#         save_feedback_to_file()\n",
    "#         print(\"Feedback stored successfully!\")\n",
    "#         return f\"Feedback for the query '{last_query}' has been stored.\"\n",
    "def submit_feedback(feedback_text):\n",
    "    \"\"\"\n",
    "    Handles feedback submission and updates the answer if the feedback indicates a wrong answer.\n",
    "    \"\"\"\n",
    "    global last_query, last_answer\n",
    "    if last_query == \"\":\n",
    "        return \"No query has been submitted yet. Please submit a query first.\"\n",
    "\n",
    "    if feedback_text.lower() in ['wrong', 'incorrect', 'not correct']:\n",
    "        # Prompt the user for the correct answer\n",
    "        corrected_answer = input(f\"Please provide the correct answer for the question '{last_query}': \")\n",
    "        if corrected_answer.strip():  # Ensure the corrected answer is not empty\n",
    "            # Store the feedback and corrected answer in the feedback file\n",
    "            feedback_data[last_query] = corrected_answer\n",
    "            save_feedback_to_file()\n",
    "            print(\"Feedback stored successfully! Answer has been corrected.\")\n",
    "            return f\"The answer has been corrected. The new answer is: {corrected_answer}\"\n",
    "        else:\n",
    "            return \"No corrected answer provided. Feedback not stored.\"\n",
    "    else:\n",
    "        # Store feedback without corrections\n",
    "        feedback_data[last_query] = last_answer\n",
    "        save_feedback_to_file()\n",
    "        print(\"Feedback stored successfully!\")\n",
    "        return f\"Feedback for the query '{last_query}' has been stored.\"\n",
    "\n",
    "\n",
    "# Gradio Blocks Setup\n",
    "with gr.Blocks() as query_block:\n",
    "    gr.Markdown(\"# Voice-based QA System with Feedback\")\n",
    "    gr.Markdown(\"Ask questions using your voice in Kannada or English, get answers, and provide feedback. If an answer is incorrect, you can provide the correct answer, and it will be stored.\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            query_input = gr.Audio(type=\"filepath\", label=\"Upload your audio query\")\n",
    "            query_button = gr.Button(\"Submit Query\")\n",
    "            query_output = gr.Textbox(label=\"Response\")\n",
    "\n",
    "        with gr.Column():\n",
    "            feedback_input = gr.Textbox(placeholder=\"Provide feedback on the answer here\", label=\"Feedback\")\n",
    "            feedback_button = gr.Button(\"Submit Feedback\")\n",
    "            feedback_output = gr.Textbox(label=\"Feedback Result\")\n",
    "\n",
    "    query_button.click(fn=handle_query, inputs=query_input, outputs=query_output)\n",
    "    feedback_button.click(fn=submit_feedback, inputs=feedback_input, outputs=feedback_output)\n",
    "\n",
    "# Launch the Gradio interface\n",
    "query_block.launch(debug=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "byteenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
